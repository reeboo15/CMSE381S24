{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfd68f5",
   "metadata": {},
   "source": [
    "# Lecture 17: Random Forests\n",
    "## CMSE 381 - Spring 2024\n",
    "\n",
    "\n",
    "![](https://creazilla-store.fra1.digitaloceanspaces.com/cliparts/63890/forest-glade-clipart-md.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90aa0b0",
   "metadata": {},
   "source": [
    "In this module we are going to set up random forest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everyone's favorite standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "\n",
    "# ML imports we've used previously\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098911f4",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9a257",
   "metadata": {},
   "source": [
    "Since we're doing regression trees, again, we're going to use the `Carseat` data where we will predict `Sales` from the rest of the columns. I'll do a bit of cleanup for you so we can get to the good stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats = pd.read_csv('../../DataSets/Carseats.csv').drop('Unnamed: 0', axis=1)\n",
    "carseats.ShelveLoc = pd.factorize(carseats.ShelveLoc)[0]\n",
    "carseats.Urban = carseats.Urban.map({'No':0, 'Yes':1})\n",
    "carseats.US = carseats.US.map({'No':0, 'Yes':1})\n",
    "carseats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1378c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = carseats.drop(['Sales'], axis = 1)\n",
    "y = carseats.Sales\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377695c",
   "metadata": {},
   "source": [
    "# Bagging \n",
    "\n",
    "Last time, we figured out how to use `sklearn.tree.DecisionTreeRegressor` to build a single tree to do our regression, but we'd like to be able to use ensemble models to create a pile of trees to give a more robust predction. \n",
    "\n",
    "Let's try doing this the hard way first. We're going to set up the bagging on our `carseats` data. We'll use $B=3$ to create our bagged model. So, step one is to generate three bootstrap samples, meaning I generate data sets by sampling $n=400$ points with replacement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4324213",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = carseats.sample(frac = 1, replace = True, random_state = 42)\n",
    "y1 = X1.Sales\n",
    "X1 = X1.drop(['Sales'], axis = 1)\n",
    "\n",
    "X2 = carseats.sample(frac = 1, replace = True, random_state = 43)\n",
    "y2 = X2.Sales\n",
    "X2 = X2.drop(['Sales'], axis = 1)\n",
    "\n",
    "X3 = carseats.sample(frac = 1, replace = True, random_state = 44)\n",
    "y3 = X3.Sales\n",
    "X3 = X3.drop(['Sales'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22288858",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Fit one regression tree of depth 2 per sampled data we just created. You'll end up with three regression trees: call them `reg_tree1`, `reg_tree2`, and `reg_tree3`. How similar are the tree structures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c13442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c08fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to generate the three trees here #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you named your models reg_tree1, reg_tree2, and reg_tree3 above, \n",
    "# the next few blocks will plot the three trees\n",
    "fig = plt.figure(figsize = (20,10))\n",
    "_= tree.plot_tree(reg_tree1, feature_names = X.columns, \n",
    "               filled = True, \n",
    "              fontsize = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc124dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,10))\n",
    "_= tree.plot_tree(reg_tree2, feature_names = X.columns, \n",
    "               filled = True, \n",
    "              fontsize = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1702681a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,10))\n",
    "_= tree.plot_tree(reg_tree3, feature_names = X.columns, \n",
    "               filled = True, \n",
    "              fontsize = 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025aa7ab",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Predict the `Sales` value for the first data point in the carseats data set using the tree(s) generated without the first data point. What is the average value of the predictions? This is the bagged prediction for this input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6990210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head().iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7830148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to get the bagged prediction for the first here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e40da41",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2acca58",
   "metadata": {},
   "source": [
    "Did you really need to do all that bagging by hand? Well, no actually, but this goes under the \"eating your vegetables exactly once\" part of the lab. Of course, `sklearn` has built in functions to do bagging for us. In reality, it just has a random forest function, but if we really want to do bagging, we can cheat. \n",
    "\n",
    "Remember, for random forests, we essentially do bagging but we only allow for a subset of $m \\leq p$ variables to be considered at each splitting step. So if we want bagging, we set the $m=$`max_features` to be the total number of features. That means this next code actually replecates what we just did above with bagging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180a90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we have p=10 input variables\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f869c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagged_carseats = RandomForestRegressor(max_features = 10, oob_score = True )\n",
    "bagged_carseats.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b2d7e",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Build a random forest model instead where the maximum number of features used at each step is $m = \\sqrt {p}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25191d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20562a",
   "metadata": {},
   "source": [
    "Because we have that `oob_score = True` set, we get access to the out of bag predictions (i.e., $\\hat y$) from `forest_carseats.oob_prediction_`. That is, for each data point, predict its $\\hat y$ by returning the averaged value using only the trees it wasn't involved in fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e3d984",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Determine the OOB error on the forest model you built just above using the `mean_squared_error` command. \n",
    "\n",
    "*Note:* The class does also keep track of the oob score as `model.oob_score_`, however this appears to be reporting $R^2$ (where close to 1 is better) and I don't seem to have the ability to change this.  See [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) for further details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0ac9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d717ab1",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** How does the number of trees used (`n_estimators` in this code) affect the error? Generate a plot like Fig 8.10 in the book for our `carseats` data. How many trees should we use? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the code below to fit a bunch of models and keep track of the MSE.\n",
    "p = 10\n",
    "\n",
    "m_list = [int(np.sqrt(p)), int(p/2), int(p)]\n",
    "print('m_list:', m_list)\n",
    "\n",
    "n_tree_list = np.arange(10,201,10)\n",
    "\n",
    "Errors = []\n",
    "for m in m_list:\n",
    "    M_error = []\n",
    "    for i in range(len(n_tree_list)):\n",
    "        n_trees = n_tree_list[i]\n",
    "        \n",
    "        #--------------------------------------------------\n",
    "        # Add code here to train a random forest model with \n",
    "        # max_features = m, n_estimators = n_trees,\n",
    "        # and the MSE saved as the error below.\n",
    "        \n",
    "        error = 0 # <----- your error goes here.\n",
    "        \n",
    "        #--------------------------------------------------\n",
    "        \n",
    "        M_error.append(error)\n",
    "    Errors.append(np.array(M_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3dba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your code above works, below you'll get a plot of the different choices of m\n",
    "colors = ['orange','blue','green']\n",
    "labels = ['sqrt(p)', 'p/2', 'p']\n",
    "\n",
    "for i in range(3):\n",
    "    M_error = Errors[i]\n",
    "    plt.plot(n_tree_list, M_error, label = labels[i], color = colors[i])\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Error in random forest for different values of $m$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8354f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
