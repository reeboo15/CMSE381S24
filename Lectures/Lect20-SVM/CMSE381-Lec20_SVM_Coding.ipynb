{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfd68f5",
   "metadata": {},
   "source": [
    "# Lec 20: SVMs\n",
    "## CMSE 381 - Spring 2024\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1920px-Kernel_Machine.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everyone's favorite standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "\n",
    "# ML imports we've used previously\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# For today, we just need SVC\n",
    "from sklearn.svm import SVC "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90aa0b0",
   "metadata": {},
   "source": [
    "We now have discussed three related methods in class. The book's definitions as we talked about in class are:\n",
    "\n",
    "- *Maximal Margin Classifiers*, where the goal was to find a separating hyperplane with no misclassifications, \n",
    "- *Support vector classifiers*, where we allow for a soft margin and hence some misclassifications, but only allow for a linear kernel, and \n",
    "- *Support vector machines*, where we have a soft margin and an option for kernels. \n",
    "\n",
    "It turns out that `sklearn` has only one function to do all of this. A reminder from last time.  **<font color=red>There are two things that will likely be confusing. </font>**\n",
    "- The command is just called `SVC`, but you should thinking of it as doing the most general SVM as defined in the book and then we can modify our inputs to allow for the other options as necessary.\n",
    "- The cost input parameter is not the same as the `C` defined in the book. However, it controls the same thing; that is, the amount of tolerance we have for data points on the wrong side of the margin and/or wrong side of the boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d05612",
   "metadata": {},
   "source": [
    "The command below is the same as from the last notebook.  The goal is just to be able to draw the boundaries from the SVM easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fdaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to define the function\n",
    "def plot_svc(svc, X, y, h=0.02, pad=0.25):\n",
    "    x_min, x_max = X[:, 0].min()-pad, X[:, 0].max()+pad\n",
    "    y_min, y_max = X[:, 1].min()-pad, X[:, 1].max()+pad\n",
    "    xvec = np.arange(x_min, x_max, h)\n",
    "    yvec = np.arange(y_min, y_max, h)\n",
    "    xx, yy = np.meshgrid(xvec,yvec )\n",
    "    \n",
    "    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.2)\n",
    "\n",
    "    plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=mpl.cm.Paired)\n",
    "    # Support vectors indicated in plot by X's\n",
    "    sv = svc.support_vectors_\n",
    "    plt.scatter(sv[:,0], sv[:,1], c='k', marker='x', s=100, linewidths=1)\n",
    "    \n",
    "    if svc.kernel == 'linear':\n",
    "        # Get the margin lines \n",
    "        w = svc.coef_[0]\n",
    "        a = -w[0] / w[1]\n",
    "        yhyperplane = a * xvec - (svc.intercept_[0]) / w[1]\n",
    "        margin = 1 / np.sqrt(np.sum(svc.coef_ ** 2))\n",
    "        ymargin_down = yhyperplane+  - np.sqrt(1 + a ** 2) * margin\n",
    "        ymargin_up = yhyperplane + np.sqrt(1 + a ** 2) * margin\n",
    "        plt.plot(xvec,ymargin_down, \"k--\")\n",
    "        plt.plot(xvec,ymargin_up, \"k--\")\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.show()\n",
    "    print('Number of support vectors: ', svc.support_.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a084b5b7",
   "metadata": {},
   "source": [
    "# Swapping out the kernel \n",
    "\n",
    "In today's class, we've been discussing changing the kernel function and then learning the model\n",
    "$$\n",
    "f(x) = \\beta_0 + \\sum_{i \\in \\mathcal{S}}\\alpha_i K(x,x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = np.loadtxt('../../DataSets/SVM-Data3.csv')\n",
    "X = data3[:,:2]\n",
    "y = data3[:,2]\n",
    "\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=mpl.cm.Paired)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421e30a",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Train a SVC using a radial kernel (this is `kernel = 'rbf'` as input to the `SVC` function) and with $C=1$, $\\gamma = 1$. Use the `plot_svc` function to see what the learned boundary looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd495c6d",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** What happens if you increase $C$ to 100? Is this model looking better or worse than what you had before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86177730",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Use the `GridSearchCV` function (see the last lab for examples of using it) to determine the best $C$ and $\\gamma$ parameters. Use the `plot_svc` function to take a look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af97c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8354f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
