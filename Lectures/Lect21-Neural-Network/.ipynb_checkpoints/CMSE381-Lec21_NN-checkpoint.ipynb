{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfd68f5",
   "metadata": {},
   "source": [
    "# Lecture 21:  Neural Nets\n",
    "## CMSE 381 - Spring 2024\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/30/Multilayer_Neural_Network.png\" alt=\"Multilayer Neural Net\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everyone's favorite standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad974880",
   "metadata": {},
   "source": [
    "Today we are going to build some basic neural nets using [pytorch](https://pytorch.org/).\n",
    "\n",
    "This lecture makes use of many helpful available tutorials, including those listed below:\n",
    "\n",
    "- https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "- https://pythonprogramming.net/data-deep-learning-neural-network-pytorch/?completed=/introduction-deep-learning-neural-network-pytorch/\n",
    "- https://towardsdatascience.com/building-neural-network-using-pytorch-84f6e75f9a\n",
    "- https://github.com/amitrajitbose/handwritten-digit-recognition/\n",
    "\n",
    "\n",
    "\n",
    "# Get up and running\n",
    "\n",
    "Your first job is to get `pytorch` running on your machine. \n",
    "\n",
    "## If you're on jupyterhub......\n",
    "You need to switch your kernel environment to `conda_pytorch`. See the figure below. Then `torch` should already be installed, no more work to be done. \n",
    "\n",
    "<img src=\"https://imgur.com/lV60kph.png\" alt=\"Jupyter hub switching to pytorch environment\" width=\"400\"/>\n",
    "\n",
    "## If you're on a local machine......\n",
    "Your first job is to install pytorch.\n",
    "```bash\n",
    "pip install torch\n",
    "```\n",
    "\n",
    "We will also be using some example data sets found in the following package. \n",
    "```bash\n",
    "pip install torchvision\n",
    "```\n",
    "\n",
    "## Either way.....\n",
    "If all goes well, the imports below should work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "# import torch.trainloader\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2730cdd",
   "metadata": {},
   "source": [
    "Our first job is to build our chosen architecture. One of the simplest ways to do this is with the `nn.Sequential` class.  All we need to do is to pass information about what we want to do at each step. The following code builds a neural network with:\n",
    "- Input of two variables $(X_1,X_2)$, so $p=2$\n",
    "- A first hidden layer with 5 units, where we take linear combinations of the inputs and then use the ReLU activation function. \n",
    "- A second hidden layer with 3 units, this time using the Sigmoid activation function\n",
    "- A final output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b80a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 2\n",
    "hidden_sizes = [5,3]\n",
    "output_size = 1\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c34267",
   "metadata": {},
   "source": [
    "Note at this point that we haven't trained anything or used data in any way.  This is only the setup. This is like when we were doing linear regression, and we have `linreg = LinearRegression()` but we hadn't done `linreg.fit(X,y)` yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af506f8f",
   "metadata": {},
   "source": [
    "\n",
    "&#9989; **<font color=red>Do this:</font>** Write code to build an architecture with the following specifications:\n",
    "- $p=20$ input variables\n",
    "- Three hidden layers, with 10, 5, and 3 units respectively.\n",
    "- Use the ReLU activation function at every step.\n",
    "\n",
    "Note you're not training the model, just setting up the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358216f8",
   "metadata": {},
   "source": [
    "# Building the simple architecture from the lecture\n",
    "<img src=\"https://imgur.com/kO6zuGG.jpg\" alt=\"Example Neural Net from Class\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa610e97",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Build the model for the example we used in the class, with the picture included above. This model had two input variables, three hidden units in a single layer, and a single output. Use ReLU for your activation function. Save your model as `mySecondNN`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f661327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988bd069",
   "metadata": {},
   "source": [
    "Here is our very simple data set to use. It's similar to the data set from last time, just with way more data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a137ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('../../DataSets/DL-toy-data-bigger.csv')\n",
    "X = data[:,:2]\n",
    "y = data[:,2]\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1], c= y)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9089a1",
   "metadata": {},
   "source": [
    "I'm going to build a train/test split before getting into the `pytorch` framework.  I know there are better internal ways to do this with pure `pytorch`, but unfortunately they aren't working for me at the moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9580bf3",
   "metadata": {},
   "source": [
    "I need to convert my input data to `pytorch`'s fancy data loader class. The first step is to conver our numpy arrays to torch Tensors, but for our purposes, you can think of this as just being a different way of storing an array. This code isn't pretty and I bet there are better ways to handle the inputs, but it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3abe1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.Tensor(X_train)\n",
    "y_train_tensor = torch.Tensor(y_train)\n",
    "X_test_tensor  = torch.Tensor(X_test)\n",
    "y_test_tensor  = torch.Tensor(y_test)\n",
    "\n",
    "mydata_train = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "mydata_test  = TensorDataset(X_test_tensor,y_test_tensor)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(mydata_train, batch_size=10, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(mydata_test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3e9b2",
   "metadata": {},
   "source": [
    "Essentially, the `trainloader` and `testloader` are now storing our data sets. The `batch_size` input is to allow for only loading in a subset of our data at a time. For our silly little data set, this doesn't particularly matter. However, for real data sets with gigabytes of data, the batch size makes it so that we don't overload the memory of the computer trying to read in the whole data set at once. \n",
    "\n",
    "Below, we can see that if we iterate over `trainloader`, we are handed 10 data points at a time, with their `X` and `y` information separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724e1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_x,data_y in trainloader:\n",
    "    print(data_x)\n",
    "    print(data_y)\n",
    "    print('---')\n",
    "#     break #<---- Uncomment to only show a single output of the iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc5979",
   "metadata": {},
   "source": [
    "Now for the actual training of the model. We are not covering the inner workings of the training in class, so for the purposes of today you don't need to worry much about the specifics here.  However, the basic idea is that `epochs` gives us the number of times we're willing to update our coefficients to see if we're improving. \"Improving\" is measured by the loss function, in this case chosen to be `nn.MSELoss` which uses mean squared error. \n",
    "\n",
    "The code below will run over multiple epochs, and print out the training loss at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b93fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "criterion = nn.MSELoss()# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(mySecondNN.parameters(), lr=0.003)\n",
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for data, target in trainloader:\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = mySecondNN(data) #<--- note this line is using the model you set up at the beginning of this section\n",
    "        output = output.float()\n",
    "        target = target.float()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f660d853",
   "metadata": {},
   "source": [
    "For a more realistic data set, we'd be looking for the training loss to be improving over time.  In our case, it's relatively stagnant since there isn't much work to be done for our particular data set.\n",
    "Note we can then predict on our test set to see how well we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = mySecondNN(X_test_tensor)\n",
    "\n",
    "criterion(predict,y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a5a1a1",
   "metadata": {},
   "source": [
    "Of course, this data set is very tiny, with not much to be done in terms of training, so lets go look at a bigger data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c653db1",
   "metadata": {},
   "source": [
    "# MNIST data set\n",
    "\n",
    "Now that we know the basics, we can build a neural net like discussed in class on the MNIST data set. The first time you run the commands below, it will save the MNIST data set into a folder called `MNIST` in the same place you're running this jupyter notebook. After that, it will just reload the data from that folder as long as it hasn't moved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6830bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test = datasets.MNIST('', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23756186",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---Train---\\n')\n",
    "print(train)\n",
    "print('\\n---Test---\\n')\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3805fd95",
   "metadata": {},
   "source": [
    "As before, we are loading in our data set in batches to keep from crashing your memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291aac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8803a60",
   "metadata": {},
   "source": [
    "Let's take a look at our data. The following code lets me spit out the first batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in trainloader:\n",
    "    print(images)\n",
    "    print(labels)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc46681",
   "metadata": {},
   "source": [
    "Note that `images` is a tensor of input data points from the first batch, while `labels` is a tensor of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98971e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdeac8c",
   "metadata": {},
   "source": [
    "This data happens to be from images of digits, so we can visualize each input data point and its label as follows. Mess around with the `i` value to see different data points in this batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44821af",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1 #<---- this number can be from 0 to 9, and will show different data points\n",
    "      #      Notice that the index i is not the same as the label of the integer in the pic.\n",
    "\n",
    "X = images[i]\n",
    "y = labels[i].item()\n",
    "\n",
    "plt.imshow(X.view(28,28))\n",
    "plt.title('This is a ' + str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed6e117",
   "metadata": {},
   "source": [
    "For our data set, we will simply flatten each image into a vector to pass into the neural network.  That means that because each image is $28 \\times 28$ pixels, we will end up with a flattened data point of size 784. The code below is taking each image from the batch, flattening it to a vector, and returning the 10 data points in the batch as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93138e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_flat = images.view(images.shape[0], -1)\n",
    "print(images_flat)\n",
    "print(images_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fd52b",
   "metadata": {},
   "source": [
    "Ok, so now we can actually train our model on MNIST! \n",
    "\n",
    "&#9989; **<font color=red>Do this:</font>** For the code below, sketch the diagram for the model we've built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2eefdb",
   "metadata": {},
   "source": [
    "Now you can run the code below to train your model! \n",
    "\n",
    "**<font color=red>Warning:</font>** This code can be pretty slow.  On my desktop, it took about 3 minutes.  You can try things like increasing the number of epochs, but note that this will also increase the running time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05030dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9890b6a",
   "metadata": {},
   "source": [
    "We can then look at what sorts of predictions we have for new data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is just for drawing.  As with the rest of this \n",
    "# tutorial, the code is adapted heavily from \n",
    "# https://github.com/amitrajitbose/handwritten-digit-recognition \n",
    "\n",
    "def view_classify(img, ps):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "    plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628765ea",
   "metadata": {},
   "source": [
    "The code below will show the image, and the probabilities for each class label. The actual prediction comes from the label with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091ac3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 4 #<--- Mess with this number to see different data points and their predictions\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "\n",
    "img = images[i].view(1, 784)\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "ps = torch.exp(logps)\n",
    "probab = list(ps.numpy()[0])\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)))\n",
    "view_classify(img.view(1, 28, 28), ps)\n",
    "# plt.savefig('MNIST-ExamplePrediction.png',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a81c1e",
   "metadata": {},
   "source": [
    "The code below will show the count  of correct predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fbf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "for images,labels in testloader:\n",
    "  for i in range(len(labels)):\n",
    "    img = images[i].view(1, 784)\n",
    "    # Turn off gradients to speed up this part\n",
    "    with torch.no_grad():\n",
    "        logps = model(img)\n",
    "\n",
    "    # Output of the network are log-probabilities, \n",
    "    # need to take exponential for probabilities\n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6390aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
